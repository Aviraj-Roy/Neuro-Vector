# Model Architecture Analysis: Complete Data Flow

This document provides a comprehensive analysis of how the three AI models work in the Medical Bill Verification system.

---

## üéØ Overview

The system uses **3 AI models** working together in a hierarchical verification pipeline:

1. **Embedding Model**: `BAAI/bge-base-en-v1.5` - Converts text to numerical vectors for similarity matching
2. **Primary LLM**: `phi3:mini` - Fast verification for borderline cases
3. **Secondary LLM**: `qwen2.5:3b` - Fallback when primary LLM fails or has low confidence

---

## 1Ô∏è‚É£ Embedding Model: `BAAI/bge-base-en-v1.5`

### **What It Does**
Converts medical text (hospital names, categories, items) into 768-dimensional numerical vectors (embeddings) that capture semantic meaning. Similar medical terms have similar vectors.

### **Configuration**
- **Location**: `.env` file, line 12
- **Full Model Name**: `BAAI/bge-base-en-v1.5` (Beijing Academy of AI)
- **Dimension**: 768 (each text becomes a 768-number vector)
- **Device**: CPU (configurable via `EMBEDDING_DEVICE`)
- **Library**: `sentence-transformers` (HuggingFace)

### **Where It Gets Input From**

#### **A. During Indexing (Startup Phase)**
**File**: `app/verifier/matcher.py` ‚Üí `SemanticMatcher.index_rate_sheets()` (lines 251-356)

**Input Sources**:
1. **Hospital Names**: From JSON rate sheets in `data/tieups/` directory
   - Example: `"Apollo Hospital"`, `"Fortis Hospital"`
   
2. **Category Names**: From each hospital's categories
   - Example: `"Consultation"`, `"Diagnostics"`, `"Surgery"`
   
3. **Item Names**: From each category's items
   - Example: `"General Physician Consultation"`, `"Blood Test - CBC"`

**Code Flow**:
```python
# Line 281-290: Index hospital names
hospital_names = [rs.hospital_name for rs in rate_sheets]
hospital_embeddings = self.embedding_service.get_embeddings_safe(hospital_names)
self._hospital_index.add(hospital_embeddings, hospital_names)

# Line 301-315: Index categories per hospital
category_names = [cat.category_name for cat in rs.categories]
category_embeddings = self.embedding_service.get_embeddings_safe(category_names)

# Line 321-335: Index items per category
item_names = [item.item_name for item in cat.items]
item_embeddings = self.embedding_service.get_embeddings_safe(item_names)
```

#### **B. During Query (Bill Verification)**
**File**: `app/verifier/matcher.py` ‚Üí `match_hospital()`, `match_category()`, `match_item()`

**Input Sources**:
- **From OCR-extracted bill data**: Hospital name, category, and item names from the uploaded PDF bill

**Code Flow**:
```python
# Line 391: Match hospital
query_embedding = self.embedding_service.get_embedding(hospital_name)

# Line 468: Match category
query_embedding = self.embedding_service.get_embedding(category_name)

# Line 557: Match item
query_embedding = self.embedding_service.get_embedding(item_name)
```

### **Where It's Processed**

**File**: `app/verifier/embedding_service.py`

**Key Functions**:

1. **Model Loading** (lines 121-173):
   ```python
   def _get_model(self):
       # Lazy-load the model from HuggingFace
       self._model = SentenceTransformer(self.model_name, device=self.device)
       self._dimension = self._model.get_sentence_embedding_dimension()  # 768
   ```

2. **Embedding Generation** (lines 195-237):
   ```python
   def _generate_embeddings(self, texts: List[str]):
       embeddings = model.encode(
           texts,
           normalize_embeddings=True,  # L2 normalization for cosine similarity
           show_progress_bar=False,
           convert_to_numpy=True,
           batch_size=32,  # Process 32 texts at once
       )
       return embeddings  # Shape: (num_texts, 768)
   ```

3. **Caching** (lines 273-346):
   - **Cache File**: `data/embedding_cache.json`
   - **Purpose**: Avoid re-computing embeddings for the same text
   - Checks cache first, only generates if not cached
   - Saves new embeddings to disk cache

### **Where It Sends Output**

**Output**: NumPy array of shape `(num_texts, 768)` or `(768,)` for single text

**Destinations**:

1. **To FAISS Index** (`app/verifier/matcher.py`, lines 289-335):
   ```python
   # Store embeddings in FAISS for fast similarity search
   self._hospital_index.add(hospital_embeddings, hospital_names)
   ```

2. **To Similarity Search** (`app/verifier/matcher.py`, lines 412, 489, 578):
   ```python
   # Find most similar hospital/category/item
   results = self._hospital_index.search(query_embedding, k=1)
   idx, similarity = results[0]  # Returns index and similarity score (0.0-1.0)
   ```

3. **To LLM Router** (indirectly via similarity score):
   - If similarity is between 0.70-0.85, triggers LLM verification

---

## 2Ô∏è‚É£ Primary LLM: `phi3:mini`

### **What It Does**
Verifies borderline medical term matches using natural language understanding. Acts as a "medical auditor" to decide if two similar-but-not-identical terms refer to the same service.

### **Configuration**
- **Location**: `.env` file, line 23
- **Model**: `phi3:mini` (Microsoft's Phi-3 Mini model)
- **Runtime**: Ollama (local inference server)
- **Base URL**: `http://localhost:11434` (Ollama API)
- **Timeout**: 30 seconds
- **Min Confidence**: 0.7

### **Where It Gets Input From**

**File**: `app/verifier/llm_router.py` ‚Üí `LLMRouter.match_with_llm()` (lines 350-460)

**Input Source**: `app/verifier/matcher.py` ‚Üí `match_item()` (lines 602-613)

**Trigger Condition**:
```python
# Line 603: Only called for borderline similarity
if use_llm and similarity >= 0.70 and similarity < 0.85:
    llm_result = self.llm_router.match_with_llm(
        bill_item=item_name,        # From bill (OCR)
        tieup_item=matched_name,    # From rate sheet (best embedding match)
        similarity=similarity,       # Embedding similarity score
    )
```

**Example Input**:
- **Bill Item**: `"Doctor Consultation - General"`
- **Tie-up Item**: `"General Physician Consultation"`
- **Similarity**: `0.78` (borderline)

### **Where It's Processed**

**File**: `app/verifier/llm_router.py`

**Processing Flow**:

1. **Check Cache** (lines 374-378):
   ```python
   cached = self._cache.get(bill_item, tieup_item)
   if cached is not None:
       return cached  # Skip LLM call if already decided
   ```

2. **Auto-Match/Reject** (lines 380-400):
   ```python
   if similarity >= 0.85:
       return LLMMatchResult(match=True, ...)  # No LLM needed
   if similarity < 0.70:
       return LLMMatchResult(match=False, ...)  # No LLM needed
   ```

3. **Build Prompt** (lines 407-411):
   ```python
   prompt = """You are a medical billing auditor.
   
   Decide if these two terms refer to the same medical service.
   
   Term A: "Doctor Consultation - General"
   Term B: "General Physician Consultation"
   
   Answer ONLY in JSON:
   {
     "match": true|false,
     "confidence": 0.0-1.0,
     "normalized_name": ""
   }
   
   No explanations. No extra text."""
   ```

4. **Call Primary LLM** (lines 413-427):
   ```python
   response_text, error = self._call_ollama(self.primary_model, prompt)
   result = self._parse_llm_response(response_text, self.primary_model)
   
   if result.is_valid and result.confidence >= 0.7:
       return result  # Success!
   ```

5. **Ollama API Call** (lines 197-231):
   ```python
   url = f"{self.base_url}/api/generate"  # http://localhost:11434/api/generate
   payload = {
       "model": "phi3:mini",
       "prompt": prompt,
       "stream": False,
       "options": {
           "temperature": 0.1,  # Low temp for deterministic output
           "num_predict": 150,  # Limit response length
       }
   }
   response = requests.post(url, json=payload, timeout=30)
   ```

### **Where It Sends Output**

**Output**: `LLMMatchResult` dataclass (lines 59-71)

**Structure**:
```python
{
    "match": True/False,           # Do the terms match?
    "confidence": 0.85,            # How confident? (0.0-1.0)
    "normalized_name": "...",      # Standardized name
    "model_used": "phi3:mini",     # Which model decided
    "error": None                  # Any error message
}
```

**Destinations**:

1. **To Matcher** (`app/verifier/matcher.py`, lines 615-626):
   ```python
   if llm_result.is_valid and llm_result.match:
       # LLM confirmed match - accept it!
       return ItemMatch(
           matched_text=matched_name,
           similarity=llm_result.confidence,  # Use LLM confidence
           index=idx,
           item=item
       )
   ```

2. **To Cache** (lines 426, 459):
   ```python
   self._cache.set(bill_item, tieup_item, result)  # Cache for future
   ```

---

## 3Ô∏è‚É£ Secondary LLM: `qwen2.5:3b`

### **What It Does**
Serves as a **fallback** when the primary LLM (`phi3:mini`) fails or returns low confidence. Provides a second opinion for difficult cases.

### **Configuration**
- **Location**: `.env` file, line 24
- **Model**: `qwen2.5:3b` (Alibaba's Qwen 2.5 3B model)
- **Runtime**: Ollama (same as primary)
- **Base URL**: `http://localhost:11434`
- **Timeout**: 30 seconds

### **Where It Gets Input From**

**File**: `app/verifier/llm_router.py` ‚Üí `LLMRouter.match_with_llm()` (lines 436-456)

**Trigger Conditions** (any of these):
1. Primary LLM API call failed
2. Primary LLM returned invalid JSON
3. Primary LLM confidence < 0.7

**Code Flow**:
```python
# Line 417-432: Try primary first
response_text, error = self._call_llm(self.primary_model, prompt)
result = self._parse_llm_response(response_text, self.primary_model)

if result.is_valid and result.confidence >= self.min_confidence:
    return result  # Primary succeeded!

# Line 436-456: Fallback to secondary
logger.info(f"Falling back to secondary model: {self.secondary_model}")
response_text, error = self._call_llm(self.secondary_model, prompt)
result = self._parse_llm_response(response_text, self.secondary_model)
```

**Input**: Same prompt as primary LLM (no changes)

### **Where It's Processed**

**File**: `app/verifier/llm_router.py`

**Processing**: Identical to primary LLM:
1. Same Ollama API call (`_call_ollama()`)
2. Same JSON parsing (`_parse_llm_response()`)
3. Same result structure (`LLMMatchResult`)

**Only Difference**: Model name changes from `phi3:mini` to `qwen2.5:3b`

### **Where It Sends Output**

**Output**: Same `LLMMatchResult` structure as primary

**Destinations**: Same as primary LLM:
1. **To Matcher** (lines 442-447)
2. **To Cache** (line 459)

**Special Case**: If secondary also fails:
```python
# Line 449-456: Both LLMs failed
result = LLMMatchResult(
    match=False,
    confidence=0.0,
    normalized_name="",
    model_used=self.secondary_model,
    error=error,
)
```

---

## üîÑ Complete Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    MEDICAL BILL VERIFICATION                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. STARTUP      ‚îÇ
‚îÇ  (Indexing)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îú‚îÄ‚Üí Load JSON rate sheets from data/tieups/
        ‚îÇ   (apollo_hospital.json, fortis_hospital.json, etc.)
        ‚îÇ
        ‚îú‚îÄ‚Üí Extract: Hospital names, Categories, Items
        ‚îÇ
        ‚îú‚îÄ‚Üí EMBEDDING MODEL (bge-base-en-v1.5)
        ‚îÇ   ‚îú‚îÄ Input: ["Apollo Hospital", "Fortis Hospital", ...]
        ‚îÇ   ‚îú‚îÄ Process: Convert to 768-dim vectors
        ‚îÇ   ‚îú‚îÄ Cache: Save to data/embedding_cache.json
        ‚îÇ   ‚îî‚îÄ Output: NumPy arrays (N, 768)
        ‚îÇ
        ‚îî‚îÄ‚Üí Build FAISS Indices
            ‚îú‚îÄ Hospital Index (all hospitals)
            ‚îú‚îÄ Category Indices (per hospital)
            ‚îî‚îÄ Item Indices (per category)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. BILL UPLOAD  ‚îÇ
‚îÇ  (Verification)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îú‚îÄ‚Üí User uploads PDF bill
        ‚îÇ
        ‚îú‚îÄ‚Üí OCR extracts: Hospital, Category, Items
        ‚îÇ
        ‚îú‚îÄ‚Üí STEP 1: Match Hospital
        ‚îÇ   ‚îú‚îÄ Input: "Apollo Hospitals Delhi"
        ‚îÇ   ‚îú‚îÄ EMBEDDING MODEL: Convert to vector (768,)
        ‚îÇ   ‚îú‚îÄ FAISS Search: Find most similar hospital
        ‚îÇ   ‚îî‚îÄ Output: "Apollo Hospital" (similarity: 0.92)
        ‚îÇ
        ‚îú‚îÄ‚Üí STEP 2: Match Category
        ‚îÇ   ‚îú‚îÄ Input: "Consultation Services"
        ‚îÇ   ‚îú‚îÄ EMBEDDING MODEL: Convert to vector (768,)
        ‚îÇ   ‚îú‚îÄ FAISS Search: Find most similar category
        ‚îÇ   ‚îî‚îÄ Output: "Consultation" (similarity: 0.88)
        ‚îÇ
        ‚îî‚îÄ‚Üí STEP 3: Match Items (for each bill item)
            ‚îÇ
            ‚îú‚îÄ Input: "Doctor Consultation - General"
            ‚îÇ
            ‚îú‚îÄ EMBEDDING MODEL: Convert to vector (768,)
            ‚îÇ
            ‚îú‚îÄ FAISS Search: Find most similar item
            ‚îÇ   ‚îî‚îÄ Result: "General Physician Consultation" (similarity: 0.78)
            ‚îÇ
            ‚îú‚îÄ DECISION LOGIC:
            ‚îÇ   ‚îú‚îÄ If similarity >= 0.85: ‚úÖ AUTO-MATCH (no LLM)
            ‚îÇ   ‚îú‚îÄ If similarity < 0.70:  ‚ùå AUTO-REJECT (no LLM)
            ‚îÇ   ‚îî‚îÄ If 0.70 <= similarity < 0.85: ü§î USE LLM
            ‚îÇ
            ‚îú‚îÄ‚Üí LLM VERIFICATION (borderline case)
            ‚îÇ   ‚îÇ
            ‚îÇ   ‚îú‚îÄ Check Cache: Already decided?
            ‚îÇ   ‚îÇ
            ‚îÇ   ‚îú‚îÄ PRIMARY LLM (phi3:mini)
            ‚îÇ   ‚îÇ   ‚îú‚îÄ Input: Prompt with both terms
            ‚îÇ   ‚îÇ   ‚îú‚îÄ Ollama API: http://localhost:11434/api/generate
            ‚îÇ   ‚îÇ   ‚îú‚îÄ Process: Medical reasoning
            ‚îÇ   ‚îÇ   ‚îî‚îÄ Output: {"match": true, "confidence": 0.85}
            ‚îÇ   ‚îÇ
            ‚îÇ   ‚îú‚îÄ If Primary Fails or Low Confidence:
            ‚îÇ   ‚îÇ   ‚îÇ
            ‚îÇ   ‚îÇ   ‚îî‚îÄ‚Üí SECONDARY LLM (qwen2.5:3b)
            ‚îÇ   ‚îÇ       ‚îú‚îÄ Input: Same prompt
            ‚îÇ   ‚îÇ       ‚îú‚îÄ Ollama API: Same endpoint
            ‚îÇ   ‚îÇ       ‚îú‚îÄ Process: Second opinion
            ‚îÇ   ‚îÇ       ‚îî‚îÄ Output: {"match": true, "confidence": 0.82}
            ‚îÇ   ‚îÇ
            ‚îÇ   ‚îî‚îÄ Cache Result: Save decision for future
            ‚îÇ
            ‚îî‚îÄ‚Üí FINAL RESULT:
                ‚îú‚îÄ If LLM confirms: ‚úÖ MATCH
                ‚îú‚îÄ If LLM rejects:  ‚ùå MISMATCH
                ‚îî‚îÄ If both fail:    ‚ùå MISMATCH (safe default)

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. OUTPUT       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚îî‚îÄ‚Üí Verification Report
            ‚îú‚îÄ Matched Items: With confidence scores
            ‚îú‚îÄ Mismatched Items: Flagged for review
            ‚îú‚îÄ Price Verification: Compare bill vs rate sheet
            ‚îî‚îÄ Statistics: LLM usage, cache hits, etc.
```

---

## üìä Model Usage Statistics

### **Embedding Model Usage**
- **Frequency**: Every single match operation
- **Typical Load**: 
  - Indexing: ~500-1000 embeddings (one-time)
  - Per Bill: ~10-50 embeddings (queries)
- **Cache Hit Rate**: ~80-90% after initial indexing

### **LLM Usage**
- **Frequency**: Only for borderline cases (0.70-0.85 similarity)
- **Typical Load**: ~10-30% of items require LLM
- **Primary vs Secondary**: ~90% resolved by primary, ~10% need secondary

### **Example Bill Verification**
For a bill with 20 items:
- **Embedding Model**: 20 calls (100%)
- **Primary LLM**: ~4 calls (20% borderline)
- **Secondary LLM**: ~0-1 calls (5% of borderline)

---

## üîß Configuration Summary

| Component | Config Variable | Default Value | Location |
|-----------|----------------|---------------|----------|
| **Embedding Model** | `EMBEDDING_MODEL` | `BAAI/bge-base-en-v1.5` | `.env:12` |
| Embedding Device | `EMBEDDING_DEVICE` | `cpu` | `.env:13` |
| Embedding Cache | `EMBEDDING_CACHE_PATH` | `data/embedding_cache.json` | `.env:16` |
| **Primary LLM** | `PRIMARY_LLM` | `phi3:mini` | `.env:23` |
| **Secondary LLM** | `SECONDARY_LLM` | `qwen2.5:3b` | `.env:24` |
| LLM Runtime | `LLM_RUNTIME` | `ollama` | `.env:25` |
| LLM Base URL | `LLM_BASE_URL` | `http://localhost:11434` | `.env:26` |
| LLM Timeout | `LLM_TIMEOUT` | `30` | `.env:27` |
| LLM Min Confidence | `LLM_MIN_CONFIDENCE` | `0.7` | `.env:28` |

---

## üìÅ Key Files Reference

### **Embedding Model**
- **Service**: `app/verifier/embedding_service.py`
- **Cache**: `app/verifier/embedding_cache.py`
- **Usage**: `app/verifier/matcher.py` (lines 215, 282, 302, 322, 391, 468, 557)

### **LLM Models**
- **Router**: `app/verifier/llm_router.py`
- **Usage**: `app/verifier/matcher.py` (lines 216, 609)

### **Integration**
- **Matcher**: `app/verifier/matcher.py` (orchestrates all models)
- **Models**: `app/verifier/models.py` (data structures)

---

## üéØ Summary

### **Embedding Model (bge-base-en-v1.5)**
- **Role**: Fast semantic similarity matching
- **Input**: Medical text strings
- **Output**: 768-dimensional vectors
- **Used**: Every match operation (100% of items)

### **Primary LLM (phi3:mini)**
- **Role**: Verify borderline matches
- **Input**: Two medical terms + similarity score
- **Output**: Match decision + confidence
- **Used**: ~20% of items (borderline cases)

### **Secondary LLM (qwen2.5:3b)**
- **Role**: Fallback for difficult cases
- **Input**: Same as primary
- **Output**: Same as primary
- **Used**: ~2% of items (when primary fails)

**Together**, these three models create a robust, accurate, and efficient medical bill verification system! üöÄ
